{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f123c733",
   "metadata": {},
   "source": [
    "# In-Painting Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148da05",
   "metadata": {},
   "source": [
    "In this small project, the foreground/background of an image is automatically segmented and can be replaced by a generated image, conditioned by a text prompt.\n",
    "\n",
    "- [SAM](#) is used as segmentation model.\n",
    "- [Stable Diffusion XL Turbo](#) is used as in-painting model.\n",
    "- [Gradio](#) is used as web UI engine.\n",
    "\n",
    "As you can see, all models and tools come from [HuggingFace](#).\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "TBD.\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Zero-Shot Image Segmenation with SAM](#zero-shot-image-segmenation-with-sam)\n",
    "- [In-Painting](#in-painting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0a47b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "481305c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image, AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52394cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA GeForce RTX 3060\n",
      "1 NVIDIA T500\n"
     ]
    }
   ],
   "source": [
    "# Name of each GPU\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f199b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed31f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  7 16:21:03 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA T500                    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P8            N/A  / 5001W |       5MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3060        On  |   00000000:52:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8             13W /  170W |    4170MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4403      G   /usr/bin/gnome-shell                      1MiB |\n",
      "|    1   N/A  N/A            4327      G   ...c/gnome-remote-desktop-daemon          2MiB |\n",
      "|    1   N/A  N/A            4403      G   /usr/bin/gnome-shell                      2MiB |\n",
      "|    1   N/A  N/A         2054628      C   ...iconda3/envs/genai/bin/python        506MiB |\n",
      "|    1   N/A  N/A         2665274      C   ...iconda3/envs/genai/bin/python       3628MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ea1bb",
   "metadata": {},
   "source": [
    "## Zero-Shot Image Segmenation with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce7e89",
   "metadata": {},
   "source": [
    "[SAM](#) is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c0dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM processor + model and move to GPU\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3824a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rgba(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transform a binary mask into an RGBA image for visualization.\"\"\"\n",
    "    bg_transparent = np.zeros(mask.shape + (4, ), dtype=np.uint8)    \n",
    "    # Color the area we will replace in green\n",
    "    # (this vector is [Red, Green, Blue, Alpha])\n",
    "    bg_transparent[mask == 1] = [0, 255, 0, 127]\n",
    "\n",
    "    return bg_transparent\n",
    "\n",
    "\n",
    "def run_segmentation(\n",
    "    image: np.ndarray,\n",
    "    input_points: np.ndarray,\n",
    "    processor: SamProcessor,\n",
    "    model: SamModel,\n",
    "    background: bool = True,\n",
    "    device: str | torch.device = \"cuda:0\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run SAM segmentation and return the best mask.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image as a NumPy array.\n",
    "        input_points (np.ndarray): The input points for SAM.\n",
    "        processor (SamProcessor): The SAM processor.\n",
    "        model (SamModel): The SAM model.\n",
    "        background (bool): Whether to mask for background or foreground.\n",
    "        device: str | torch.device = \"cuda:0\", \n",
    "            The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The best mask as a NumPy array.\n",
    "    \"\"\"\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        input_points=input_points,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Post-process the outputs of SAM to obtain the masks\n",
    "    masks = processor.image_processor.post_process_masks(\n",
    "       outputs.pred_masks.cpu(), \n",
    "       inputs[\"original_sizes\"].cpu(), \n",
    "       inputs[\"reshaped_input_sizes\"].cpu()\n",
    "    )\n",
    "    \n",
    "    # Select the mask with the highest score\n",
    "    best_mask = masks[0][0][outputs.iou_scores.argmax()] \n",
    "\n",
    "    # Invert the mask if we want to mask the background:\n",
    "    # subject pixels will have a value of 0 \n",
    "    # and the background pixels a value of 1.\n",
    "    return ~best_mask.cpu().numpy() if background else best_mask.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f140576",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1c4f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAKzElEQVR4Ae2c3a8dVRnG97anUGgPPT1tseU7oFCR2KKxUYgXBhFJ4IKEkIAJ3HhDwv/An+GN0YumF8YQgwlBiVHUxCjBJuJHikprG0EE2p7Tz9PTwvD8Zvaqu/vM3vudfebsM7PW+yZP18yad9a8H89618zs6el2ss6LHZdkI/CZZD13x/MIOAESJ4ITwAmQeAQSd98rgBMg8Qgk7r5XACdA4hFI3H2vAE6AxCOQuPteAZwAiUcgcfe9AjgBEo9A4u57BXACJB6BxN33CuAESDwCibvvFcAJkHgEEnffK4ATIPEIJO6+VwAnQOIRSNx9rwBOgMQjkLj7XgESJ8BMRP535QsYlH6SZzr4iUDrogg0jQCDSRy2HxLZn8Tz2vm4r4Nz2T8hkHTkGmG7cL2wQeB4uAbtKAmkCdcO+6POafyxugkQghkcL9vnGP0hKf2BvKz+CyhI0DkpLPe26f9AQEjce0IYg75TArr9kmmUMB7znjE3CzuFu4UvCQsCOoPjqeuKbNXWnICt/SSiumBza2W1BCAAICTxorZDwgjMOSEkgOCfFZZ6/f3b6sqTw/kkMsgF9YaxQ9/kbTEW1wVHNfJv1S5VusbVJPqyzt8rYGM/GbXbDplZhZmU0DPCr4STAglmn6QjBGRZvc0NTH91yE02/LOSRP/QWQ8L88IlwwiNUpmUAMx6Ev1jJfh4ozyatjHdzl80/4kBJNgnUPnqq1oabC2FRFYVZjpMP5h88kPkup1Fbb4k/FKAAFTHVsgkBMA5bpjebYWH0zKSpaHbeV2XOyCcETYKjZdJCEAFOCxnW1PmppqFbueYrvcj4YjQeBJUJQD63EH/VXAZFoFu5yMdOigcEiABk6aRMgkB3pA7pxvpTZOM6nZ4pP2p8DuBatlIElQhAA6cF/4kuFgiUNwX/EKqgBdIjSNBFQKguyicE1yqReANqb8mNK4SVCXAP8VhHnNcqkSAl2HFE8LPddoGoTGVwEoAL/9VEj5Mt9v5ow693Dtsjf2w0Wrpt74JxNgF4XQtV015kG7nTS0ELAXfFq4TiC0TjD6A9G+zv2av06sQwMs/qahDurqRzjr/0lA7hd0CywJEuKm3zS+WWwSIADnYp0UHgRCBJIE0+YGq/1gJwM+vGOxSVwSK18eLGm5lXLP8Ow2eGhCSvl3gfQJkgQy3CtsE+jYJCKSoXCksBMAA7vw/FFymEYHiRvty36XO9rYLshQ/SbN8QAaqyF4BUmwRmKxmsRCA0nNS4B2ASxMiULyGJx+Aifl3LQizah8TviCYSRDWFJ0zVCAAH2ZULi9DR/QD9Uegm/8A9RMN/F+BewqTWAnwvmk0V1rfCBRLx+tVjLAQgPG4B3BpRwSOysxTgim345Qo/5eE44JLGyLQzb+5/JtMNS0DFgIw+7kJdGlPBP4sU/n4lgk8UsYRgJcMPGrwgsKlPRFYlKnLAu8SRpLAQoCNGmS/4NKeCFyUqfzw9AcBIgwlwTgC6Nz8leMmNlxaEoHiO4S3lPafyeKXRlltIQDseX/UIH6swRHodt6WdceE0pd+FgLgXfjfPWy7tC8Cx2Vy6TJgJUDpye2LQ7IWL8hzbuhXiJUAK070jlZF4HpZWzqJnQCtyuPExvKj0cQVgBN3THxpP7EJEVhVBaB0+JvAJqRxchu4iZ+4AnBZPj5waW8Ehr4R9HuA9ia1iuX/k3Lp9xwWAlA65qtczXXbEwErAWa1gvCbgEs7I7BbZpfmurRzwEcqAB8bOgEGAtOK3eIL43tk66qWAG4Ct7XCYTdyMAKPqOMOofRD0dIfCAZGoAKg578IDgSm0btZ50bZ96CwTyhNvvrLfyHiQInsU987Jf3e1ZQIZPlS/TWZQ7XeI7BsD02+jpkJQBXYpRvBDXqjPHJABnVZtwgw6x8SLgvkaWyuLDeBGie/gdip9k52XBobASYqH/FCALbHiuUeIPyKhO6jGvZIslXg//8li4lzg8A7dp6Q+G6SOF0U3tbWabWtkHEEwNFfCzDqAWFOeEIk+L2cfE/b8UmWJ3OvHDsqLAj3CvPCTQJrKtsIN8XEj8+vjwjHhH8LZwTmH09OHO9OkRBhsmKBSUYRgMH4EeEdgRcJB4RnhfuEm+XgD+VY4aw6GitFImZlHyWR2UriWM7OCbcJbwmUTWJB0pjV+4UlYYfwTWGb8InAGAHavLLO3qLtXcL9wmFpcJ2bhWuFQ8KrwjTkLl2ESTt27Q/G8Pngi2GnpMXZE8INAkEiOAgXWRSoAhCEYBHQywKk2CCc7bUcYx4QwPqleNGxSQPP6BoLV10g62zV/ncFEklQmMHY1hXwjZayTR8+sR8SzTaCHhgn6APGQRiHfcY/oK3jakdLlk+0rdI9PFqx5GixPH1PR24TyINJZsZo4QB3lgSAIIcksj8nzAv3CQh9BHlZIAgQ4hrhP8LHOvobtfwn00W19UiWzzASvEs4KCwIhWT5n4N/QjvMagSbsHEwOCQ/2K7NK0JfFUEfhBiFc6kCT+rID+T76dC5os3yvwHwnPpnpPt96TLxqgiJZ5ka9G/kGARlnOBQmWP0c7EAko/gMDNtm7BZ2CN8UXheeEioU0jeToHr3Hll4KLsP679LUJIDG2ZDOsv052kj7jMCc/KkruF7lWDYGuW+wCRqbBMmqfVR2sTHs+LPzlDW0nGVYBKg/WUQ0BDG8bgWrvl2EaF4FLorKGFiAT5QY39oVr2vy7MCpVmg/TXSrCPSvqM8K7s/EgtlYkkb++1VFj0EHTvl94hY6yYbEwEfK8ka0GAYQZgHAznmnUSgOtBNsZ9ip2ehGCG/fVugz23ypDbhTBBiAvb/clD9zFhj47wV9mHEznLKy3EovL2j6Hd8TJNAuAkRs4KF8abZtJgCQsllfFDkE0nr5MSNlrsJOl3Cc+IBG+qpbqdE4JQPagU3xGY/cNJooPDZNoEoMxRnl8eZlDF/nnpbxYqM7/iddZLHaJ8vocltct9hrDeU1Eh/kTJZyzLTSB6dQmG7pPJn6tpwFs0TuUbn5quPa1hiBnYKGzpQ7hnWBX5p00A2Mo1eZv4WbWTS/EI+FUNsKoATG7A1M8kdvgawP6qZdoEwGAcmBWeEwkeFijhkwhLyQ7Bsp5OMn4S56wHAQgsSSPx3xBeEAm+otYuWf5Sar9OSGX222NTUXO9CICZJC8Q4RGRYLvJ9ixfQr4lXdZDJ4ApaMOV1pMAwSpIwOMhS8LtoXNE+6iOUf45z2WVEZjmY+AoU5nJcwLPvK+oPSGc0hP+ee3zvIudVIgHhHuFS4JLDRFoCgFwJVSCJ7XNY8+Skn9SLc+6PPJcJ2Avx1xqikCTCIBLPNpABJYmbhJZ58PjDq0nX0GoU5pGgOAbyQ6JD33erkEEmnATuAZu+ZDWCDgBrJGKVM8JEGlirW45AayRilTPCRBpYq1uOQGskYpUzwkQaWKtbjkBrJGKVM8JEGlirW45AayRilTPCRBpYq1uOQGskYpUzwkQaWKtbjkBrJGKVM8JEGlirW45AayRilTPCRBpYq1uOQGskYpUzwkQaWKtbjkBrJGKVM8JEGlirW45AayRilTPCRBpYq1uOQGskYpUzwkQaWKtbjkBrJGKVM8JEGlirW45AayRilTPCRBpYq1uOQGskYpUzwkQaWKtbjkBrJGKVM8JEGlirW45AayRilTPCRBpYq1ufQqbeO/Zy7MvkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=128x128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example image\n",
    "raw_image = Image.open(\"./assets/car.png\").convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Coordinates of two points on the car\n",
    "input_points = [[[150, 170], [300, 250]]]\n",
    "\n",
    "# Run segmentation\n",
    "mask = run_segmentation(raw_image, input_points, sam_processor, sam_model, device)\n",
    "\n",
    "# Visualize the mask\n",
    "Image.fromarray(mask_to_rgba(mask)).resize((128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b482f",
   "metadata": {},
   "source": [
    "## In-Painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "101e194c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8921adb6b14a4ff388e8a5c93e01fef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "# Load the AutoPipelineForInpainting pipeline\n",
    "inpainting_pipeline = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# This will make it more efficient on our hardware\n",
    "inpainting_pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0eb0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inpainting(\n",
    "    raw_image: np.ndarray,\n",
    "    input_mask: np.ndarray,\n",
    "    prompt: str,\n",
    "    pipeline: AutoPipelineForInpainting,\n",
    "    negative_prompt: str = None,\n",
    "    seed: int = 74294536,\n",
    "    cfgs: int = 7,\n",
    "):\n",
    "    mask_image = Image.fromarray(input_mask)\n",
    "\n",
    "    rand_gen = torch.manual_seed(seed)\n",
    "    \n",
    "    # Use the pipeline we have created in the previous cell\n",
    "    # Use \"prompt\" as prompt, \n",
    "    # \"negative_prompt\" as the negative prompt,\n",
    "    # raw_image as the image,\n",
    "    # mask_image as the mask_image,\n",
    "    # rand_gen as the generator and\n",
    "    # cfgs as the guidance_scale\n",
    "    image = pipeline(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=raw_image,\n",
    "        mask_image=mask_image,\n",
    "        generator=rand_gen,\n",
    "        guidance_scale=cfgs,\n",
    "    ).images[0]\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f9e03",
   "metadata": {},
   "source": [
    "## Interactive Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75fd67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0de4aa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AnnotatedImage.__init__() got an unexpected keyword argument 'interactive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_app \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_segmentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_inpainting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_repositories/diffusion-examples/inpainting_app/app.py:153\u001b[0m, in \u001b[0;36mgenerate_app\u001b[0;34m(get_processed_inputs, inpaint)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mRow():\n\u001b[1;32m    143\u001b[0m     \n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# This is what the user will interact with\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     display_img \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mImage(\n\u001b[1;32m    146\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    147\u001b[0m         interactive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         width\u001b[38;5;241m=\u001b[39mIMG_SIZE\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     sam_mask \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAnnotatedImage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSAM result\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minteractive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#a89a00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     result \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mImage(\n\u001b[1;32m    162\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m         interactive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m         width\u001b[38;5;241m=\u001b[39mIMG_SIZE,\n\u001b[1;32m    167\u001b[0m     )            \n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Events\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.10/site-packages/gradio/component_meta.py:188\u001b[0m, in \u001b[0;36mupdateable.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: AnnotatedImage.__init__() got an unexpected keyword argument 'interactive'"
     ]
    }
   ],
   "source": [
    "my_app = app.generate_app(run_segmentation, run_inpainting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97751861",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
