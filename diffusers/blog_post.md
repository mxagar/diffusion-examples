# Image Generation with Diffusion

<!--
# Log in/out to Docker Hub
docker logout
docker login

# Pull the official image (first time)
docker pull excalidraw/excalidraw

# Start app
docker run --rm -dit --name excalidraw -p 5000:80 excalidraw/excalidraw:latest
# Open browser at http://localhost:5000

# Stop
docker stop excalidraw
docker rm excalidraw
docker ps

-->

<div style="height: 20px;"></div>
<div align="center" style="border: 1px solid #e4f312ff; background-color: #fcd361b9; padding: 1em; border-radius: 6px;">
<strong>
This is the second post of a series of two.
You can find the <a href="https://mikelsagardia.io/posts/">first part here</a>.
Also, you can find the accompanying code in <a href="https://github.com/mxagar/diffusion-examples/diffusers">this GitHub repository</a>.
</strong>
</div>
<div style="height: 30px;"></div>


Blog Post 1  
Title: An Intorduction to Image Generation with Diffusers (1/2)  
Subtitle: A Conceptual Guide for Developers

Blog Post 2  
Title: An Intorduction to Image Generation with Diffusers (2/2)  
Subtitle: Hands-On Examples with Hugging Face

<p align="center">
<img src="../assets/ai_drawing_ai_dallev3.png" alt="An AI drawing an AI drawing an AI. Image generated using Dalle-E v3" width="1000"/>
<small style="color:grey">An AI drawing an AI drawing an AI... Image generated using 
<a href="https://openai.com/index/dall-e-3/">Dall-E v3</a>. Prompt: <i>A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook. Soft light colors, landscape, peaceful, productive, and joyful atmosphere. The robot is drawing an image of itself drawing, creating a recursive effect. Large window in the background with greenery outside, warm natural lighting.</i>
</small>
</p>

In very few years, image generation has become an almost ubiquitous tool. [Variational Autoencoders (VAEs - )](#) were followed by [Generative Adversarial Networks (GANs - )](#), and finally [Denoising Diffusion Probabilistic Models (DDPMs - Ho et al., 2020)]([#](https://arxiv.org/abs/2006.11239)) conquered the landscape, with remarkable models like [Stable Diffusion XL (Podell et al., 2023)]([#](https://arxiv.org/pdf/2307.01952)). In the [first post of this series](#) I explain how each of these models works and I provide an example implementation of a DDPM, which is trained to run car image generation, producing these examples:

<p align="center">
<img src="../assets/car_generation_best_model.png" alt="Eight Samples Generated by a DDPM" width="1000"/>
<small style="color:grey">
Output of a <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Model (Ho et al., 2020)</a> consisting of 54 million paramaters, trained on the <a href="https://www.kaggle.com/datasets/eduardo4jesus/stanford-cars-dataset">Stanford Cars Dataset</a> (16,185 color images resized to <code>64x64</code> pixels) for 300 epochs. Check the complete implementation <a href="https://github.com/mxagar/diffusion-examples/tree/main/ddpm">here</a>.
</small>
</p>

In this second part, I would like to focus on **the practical application** of the diffusers, in particular, using the invaluable tools provided by [HuggingFace](https://huggingface.co/). To that end, I hava devided the post in three parts:

1. First, I provide a brief [Introduction to HuggingFace](#introduction-to-huggingface);
2. Then, I deep-dive into some examples with [HuggingFace Diffusers](#huggingface-diffusers);
3. And finally, I show how to build an [In-Painting Application](#in-painting-application) using the aforementioned tools.

Let's go!

## A Very Brief Introduction to HuggingFace

[HuggingFace](https://huggingface.co) has become one of the most important portals in the machine learning community. It builds a collaborative environment where state-of-the-art **datasets** and **models** can be **shared** and **tried** (*Spaces*). Moreover, HuggingFace offers two additional and very powerful sets tools:

- [**courses**](https://huggingface.co/learn) which deal with the most important domains and technical methods: computer vision, natural language processing, audio, agents, 3D processing, reinforcement learning, etc.;
- and **libraries** with which we can handle both datasets and models end-to-end and for any relevant modality. The most important ones are:
    - [`datasets`](https://huggingface.co/docs/datasets/en/index): conceived to access and share audio, text, and image data.
    - [`transformers`](https://huggingface.co/docs/transformers/en/index): for model training and inference, supporting text, computer vision, audio, video, and multimodal data.
    - [`diffusers`](https://huggingface.co/docs/diffusers/en/index): pretrained diffusion models for generating videos, images, and audio.

<p align="center">
<img src="../assets/hugging_face_screenshot.png" alt="HuggingFace Screeshot" width="1000"/>
<small style="color:grey">
Screeshot of the <a href="https://huggingface.co">HuggingFace</a> portal, featuring available models sorted by their popularity.
</small>
</p>


Discriminative models are handled usually by `transformers`, as well generative models for text. On the other hand, generative *diffusion* models are contained in `diffusers`. Browsing and selecting a model can be done on the 



```bash
pip install datasets transformers diffusers["torch"]
```



```python
from diffusers import ConcreteModel

# Load the pipeline
pipe = ConcreteModel.from_pretrained(
    "Organization/ConcreteModel",
    ...
)

# Define Prompt
prompt = "An AI drawing an AI"

# Generate Image
image = pipe(
    prompt=prompt,
    ...
).images[0]

# Save image
image.save("example.png")
```


```python
import transformers

# Load the pipeline
pipeline = transformers.pipeline(
    "text-generation",
    model="Organization/ConcreteModel",
)

# Define Prompt
messages = [
    {"role": "system", "content": "You are an AI who can draw AIs."},
    {"role": "user", "content": "What's the best technique to draw an AI?"},
]

# Generate Text
outputs = pipeline(
    messages,
)

# Display Text
print(outputs[0]["generated_text"][-1])
```


- [HuggingFace Guide](https://github.com/mxagar/tool_guides/tree/master/hugging_face)
- [HuggingFace LLM Fine-Tuning](https://github.com/mxagar/llm_peft_fine_tuning_example)
- [Natural Language Processing with Transformers: My Notes](https://github.com/mxagar/nlp_with_transformers_nbs)


## HuggingFace Diffusers

:construction: TBD.

[`diffusers_and_co.ipynb`](https://github.com/mxagar/diffusion-examples/blob/main/diffusers/diffusers_and_co.ipynb)

```python
from diffusers import AutoPipelineForText2Image

# Load the SDXL-Turbo text-to-image pipeline
pipe = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/sdxl-turbo", 
    torch_dtype=torch.float16, 
    variant="fp16"
).to(device)

prompt = """
A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook.
Soft light colors, landscape, peaceful, productive, and joyful atmosphere.
The robot is drawing an image of itself drawing, creating a recursive effect.
Large window in the background with greenery outside, warm natural lighting.
"""

# Seed for reproducibility
rand_gen = torch.manual_seed(148607185)

# Generate an image based on the text prompt
image = pipe(
    prompt=prompt, 
    num_inference_steps=1, # For this model you can use 1, but for normal Stable Diffusion you should use 25 or 50
    guidance_scale=1.0, # For this model 1 is fine, for normal Stable Diffusion you should use 6 or 7, or up to 10 or so
    negative_prompt=["overexposed", "underexposed"], 
    generator=rand_gen
).images[0]
```


<p align="center">
<img src="../assets/robot_painting_sdxl_turbo.png" alt="A friendly humanoid robot drawing itself." width="1000"/>
<small style="color:grey">
Model: <a href="https://huggingface.co/stabilityai/sdxl-turbo">SDLX Tubo</a>.
Prompt: <i>A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook. Soft light colors, landscape, peaceful, productive, and joyful atmosphere. The robot is drawing an image of itself drawing, creating a recursive effect. Large window in the background with greenery outside, warm natural lighting.</i>
</small>
</p>

<p align="center">
<img src="../assets/robot_painting_playground_v2.png" alt="A friendly humanoid robot drawing itself." width="1000"/>
<small style="color:grey">
Model: <a href="https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic">Playground V2</a>.
Same prompt ad before: <i>A friendly humanoid robot sits at a wooden table...</i>
</small>
</p>


<p align="center">
<img src="../assets/dog_drawing_sdlx_turbo_kandinsky.png" alt="A friendly humanoid dog." width="1000"/>
<small style="color:grey">
Model: <a href="https://huggingface.co/stabilityai/sdxl-turbo">SDLX Tubo</a>.
Model: <a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-prior">Kandinsky Prior 2.2</a>.
Prompt (left): <i>A painting of a friendly dog painted by a child.</i>
Prompt (left): <i>A photo of a friendly dog. High details, realistic (negative: low quality, bad quality).</i>
</small>
</p>


<p align="center">
<img src="../assets/vermeer_girl_mask_inpainting_kandinsky.png" alt="A friendly humanoid dog." width="1000"/>
<small style="color:grey">
Model: <a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint">Kandinsky Inpaint 2.2</a>.
Prompt (left): <i>Oil painting of a woman wearing a surgical mask, Vermeer (negative: bad anatomy, deformed, ugly, disfigured).</i>
</small>
</p>


## In-Painting Application

:construction: TBD.

## Conclusions

:construction: TBD.
