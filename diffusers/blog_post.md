# Image Generation with Diffusion

<!--
# Log in/out to Docker Hub
docker logout
docker login

# Pull the official image (first time)
docker pull excalidraw/excalidraw

# Start app
docker run --rm -dit --name excalidraw -p 5000:80 excalidraw/excalidraw:latest
# Open browser at http://localhost:5000

# Stop
docker stop excalidraw
docker rm excalidraw
docker ps

-->

<div style="height: 20px;"></div>
<div align="center" style="border: 1px solid #e4f312ff; background-color: #fcd361b9; padding: 1em; border-radius: 6px;">
<strong>
This is the second post of a series of two.
You can find the <a href="https://mikelsagardia.io/posts/">first part here</a>.
Also, you can find the accompanying code in <a href="https://github.com/mxagar/diffusion-examples/diffusers">this GitHub repository</a>.
</strong>
</div>
<div style="height: 30px;"></div>


Blog Post 1  
Title: An Intorduction to Image Generation with Diffusers (1/2)  
Subtitle: A Conceptual Guide for Developers

Blog Post 2  
Title: An Intorduction to Image Generation with Diffusers (2/2)  
Subtitle: Hands-On Examples with Hugging Face

<p align="center">
<img src="../assets/ai_drawing_ai_dallev3.png" alt="An AI drawing an AI drawing an AI. Image generated using Dalle-E v3" width="1000"/>
<small style="color:grey">An AI drawing an AI drawing an AI... Image generated using 
<a href="https://openai.com/index/dall-e-3/">Dall-E v3</a>. Prompt: <i>A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook. Soft light colors, landscape, peaceful, productive, and joyful atmosphere. The robot is drawing an image of itself drawing, creating a recursive effect. Large window in the background with greenery outside, warm natural lighting.</i>
</small>
</p>

In very few years, image generation has become an almost ubiquitous tool. [Variational Autoencoders (VAEs - )](#) were followed by [Generative Adversarial Networks (GANs - )](#), and finally [Denoising Diffusion Probabilistic Models (DDPMs - Ho et al., 2020)]([#](https://arxiv.org/abs/2006.11239)) conquered the landscape, with remarkable models like [Stable Diffusion XL (Podell et al., 2023)]([#](https://arxiv.org/pdf/2307.01952)). In the [first post of this series](#) I explain how each of these models works and I provide an example implementation of a DDPM, which is trained to run car image generation, producing these examples:

<p align="center">
<img src="../assets/car_generation_best_model.png" alt="Eight Samples Generated by a DDPM" width="1000"/>
<small style="color:grey">
Output of a <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Model (Ho et al., 2020)</a> consisting of 54 million paramaters, trained on the <a href="https://www.kaggle.com/datasets/eduardo4jesus/stanford-cars-dataset">Stanford Cars Dataset</a> (16,185 color images resized to <code>64x64</code> pixels) for 300 epochs. Check the complete implementation <a href="https://github.com/mxagar/diffusion-examples/tree/main/ddpm">here</a>.
</small>
</p>

In this second part, I would like to focus on **the practical application** of the diffusers, in particular, using the invaluable tools provided by [HuggingFace](https://huggingface.co/). To that end, I hava devided the post in three parts:

1. First, I provide a brief [Introduction to HuggingFace](#introduction-to-huggingface);
2. Then, I deep-dive into some examples with [HuggingFace Diffusers](#huggingface-diffusers);
3. And finally, I show how to build an [In-Painting Application](#in-painting-application) using the aforementioned tools.

Let's go!

## Introduction to HuggingFace

:construction: TBD.

## HuggingFace Diffusers

:construction: TBD.

[`diffusers_and_co.ipynb`](https://github.com/mxagar/diffusion-examples/blob/main/diffusers/diffusers_and_co.ipynb)

## In-Painting Application

:construction: TBD.

## Conclusions

:construction: TBD.
