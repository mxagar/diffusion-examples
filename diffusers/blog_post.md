# Image Generation with Diffusion
<!--
# Log in/out to Docker Hub
docker logout
docker login

# Pull the official image (first time)
docker pull excalidraw/excalidraw

# Start app
docker run --rm -dit --name excalidraw -p 5000:80 excalidraw/excalidraw:latest
# Open browser at http://localhost:5000

# Stop
docker stop excalidraw
docker rm excalidraw
docker ps

---

Blog Post 1  
Title: An Introduction to Image Generation with Diffusion Models (1/2)  
Subtitle: A Conceptual Guide for Developers and ML Practitioners

Blog Post 2  
Title: An Intorduction to Image Generation with Diffusion Models (2/2)
Subtitle: Hands-On Examples with Hugging Face
-->
<p align="center">
<img src="../assets/ai_drawing_ai_dallev3.png" alt="An AI drawing an AI drawing an AI. Image generated using Dalle-E 3" width="1000"/>
<small style="color:grey">An AI drawing an AI drawing an AI... Image generated using 
<a href="https://openai.com/index/dall-e-3/">Dall-E v3</a>. Prompt: <i>A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook. Soft light colors, landscape, peaceful, productive, and joyful atmosphere. The robot is drawing an image of itself drawing, creating a recursive effect. Large window in the background with greenery outside, warm natural lighting.</i>
</small>
</p>

<div style="height: 20px;"></div>
<div align="center" style="border: 1px solid #e4f312ff; background-color: #fcd361b9; padding: 1em; border-radius: 6px;">
<strong>
This is the second post of a series of two.
You can find the <a href="https://mikelsagardia.io/blog/diffusion-for-developers.html">first part here</a>.
Also, you can find the accompanying code in <a href="https://github.com/mxagar/diffusion-examples/tree/main/diffusers">this GitHub repository</a>.
</strong>
</div>
<div style="height: 30px;"></div>

In just a few years, image generation has gone from *"cool demo"* to an almost ubiquitous tool. [Variational Autoencoders (VAEs - Kingma & Welling, 2013)](https://arxiv.org/abs/1312.6114) were followed by [Generative Adversarial Networks (GANs - Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661), and finally [Denoising Diffusion Probabilistic Models (DDPMs - Ho et al., 2020)](https://arxiv.org/abs/2006.11239) became the dominant approach, leading to systems like [Stable Diffusion XL (Podell et al., 2023)](https://arxiv.org/pdf/2307.01952) or [Imagen & Nano Banana](https://arxiv.org/abs/2205.11487).

In the [first post of this series](https://mikelsagardia.io/blog/diffusion-for-developers.html), I explain how these model families work and I walk through a minimal DDPM implementation.
That DDPM is trained on car images and produces outputs like these:

<p align="center">
<img src="../assets/car_generation_best_model.png" alt="Eight Samples Generated by a DDPM" width="1000"/>
<small style="color:grey">
Output of a <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Model (Ho et al., 2020)</a> consisting of 54 million parameters, trained on the <a href="https://www.kaggle.com/datasets/eduardo4jesus/stanford-cars-dataset">Stanford Cars Dataset</a> (16,185 color images resized to <code>64x64</code> pixels) for 300 epochs. Check the complete implementation <a href="https://github.com/mxagar/diffusion-examples/tree/main/ddpm">here</a>.
</small>
</p>

In this second part, I'll focus on **the practical use of diffusion models**, specifically, on using the invaluable tools provided by [HuggingFace](https://huggingface.co/). To that end, I've divided the post in three parts:

1. A brief [introduction to HuggingFace](#introduction-to-huggingface).
2. A hands-on dive into some examples with [HuggingFace Diffusers](#huggingface-diffusers).
3. A small [in-painting application](#in-painting-application) that puts everything together.

Let's go!

## A Very Brief Introduction to HuggingFace

[HuggingFace](https://huggingface.co) has become one of the most important hubs in the machine learning community. It provides a collaborative environment where state-of-the-art **datasets** and **models** can be **shared**, **explored** and even **tried** directly from the browser (via *Spaces*). Beyond models and datasets, HuggingFace offers two particularly powerful resources:

- [**courses**](https://huggingface.co/learn), covering key domains and techniques such as computer vision, natural language processing, audio, agents, 3D processing, reinforcement learning, and more;
- and a rich ecosystem of **libraries** that let us work with datasets and models end-to-end, across modalities. The most relevant ones for this post are:
    - [`datasets`](https://huggingface.co/docs/datasets/en/index): access, share, and process audio, text, and image datasets.
    - [`transformers`](https://huggingface.co/docs/transformers/en/index): training and inference for text, vision, audio, video, and multimodal models.
    - [`diffusers`](https://huggingface.co/docs/diffusers/en/index): pretrained diffusion models for generating images, videos, and audio.

All of these can be installed easily in a Python environment:

```bash
pip install datasets transformers diffusers["torch"] accelerate gradio
```

<p align="center">
<img src="../assets/hugging_face_screenshot.png" alt="HuggingFace Screeshot" width="1000"/>
<small style="color:grey">
Screeshot of the <a href="https://huggingface.co">HuggingFace</a> portal, showing available models sorted by their popularity.
</small>
</p>

In practice, *discriminative* models (across all modalities) and *generative* models for text are usually handled via the `transformers` library. On the other hand, generative *diffusion* models are managed through `diffusers`.

Models can be browsed and selected directly from the HuggingFace website, where they can be filtered by several criteria. One of the most useful is the [task](https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.pipeline.task), for example:

- `sentiment-analysis`
- `text-generation`
- `summarization`
- `translation`
- `audio-classification`
- `image-to-text`
- `object-detection`
- `image-segmentation`
- ...

If we click on a model we will land on its **model card** page, which typically includes evaluation metrics, references, licensing information, and often a short code snippet showing how to load and run the model.

The easiest way to run inference with most HuggingFace models is through the `pipeline` interface. While each task has its own specifics, the overall pattern is remarkably consistent. As an example, here's how a `text-generation` pipeline looks:

```python
import transformers

# Load the pipeline
pipe = transformers.pipeline(
    "text-generation",  # task
    model="Organization/ConcreteModel",  # change to real model, e.g.: "openai-community/gpt2"
)

# Define the input (prompt)
messages = [
    {"role": "system", "content": "You are an AI who can draw AIs."},
    {"role": "user", "content": "What's the best technique to draw an AI?"},
]

# Generate output (text)
outputs = pipe(
    messages,
)

# Display output (text)
print(outputs[0]["generated_text"][-1])
```

From this (deliberately simplified) example, we can extract a common workflow:

- First, a model pipeline is loaded, by defining the task family (e.g., `text-generation`) as well as the concrete model name (e.g., `openai-community/gpt2`) we want to use.
- Then, we need to define the input to the pipeline; the input depends on the task at hand: if we want to classify an image, we need to load an image; if we want to generate text, we need an initial prompt of conversation history, etc.
- Finally, we pass the input to the pipeline and collect the output. The output format again depends on the task.

Instead of relying on the generic `pipeline` abstraction, we can also load a specific model class directly. This is particularly common when working with diffusion models. For example, a typical `text-to-image` setup using `diffusers` looks like this:

```python
from diffusers import ConcreteModel   # change to real model, e.g.: AutoPipelineForText2Image

# Load the pipeline
pipe = ConcreteModel.from_pretrained(
    "Organization/ConcreteModel",  # change to real model, e.g.: "stabilityai/sdxl-turbo"
    ...
)

# Define the input (prompt)
prompt = "An AI drawing an AI"

# Generate output (image)
image = pipe(
    prompt=prompt,
    ...
).images[0]

# Save output (image)
image.save("example.png")
```

This brief overview barely scratches the surface of the HuggingFace ecosystem. In the next sections, I'll focus on concrete, ready-to-use examples that build directly on these ideas.

If you'd like to explore further, here are some additional resources:

- My [guide on HuggingFace](https://github.com/mxagar/tool_guides/tree/master/hugging_face), which covers topics such as:
    - Combining models with Pytorch/Tensorflow code.
    - More complex pre- and post-processing steps for each task/modality, e.g.: tokenization, encoding, etc.
    - Fine-tuning pretrained models for different tasks by adding cutom heads. 
    - Saving/loading fine-tuned models locally, as well as exporting them as ONNX for production.
    - Examples with genertive models of all modalities and conditioning types: `text-generation`, `text-to-image`, `text-to-video`, etc.
- A comprehensive example in which I [fine-tune a Large Language Model (LLM)](https://github.com/mxagar/llm_peft_fine_tuning_example) to perform a custom text classification task.
- My notes on the exceptional book [Natural Language Processing (NLP) with Transformers (Tunstall, von Werra & Wolf &mdash; O'Reilly)](https://github.com/mxagar/nlp_with_transformers_nbs), written by the co-founders of HuggingFace &mdash; highly recommended if you want to use `transformers` effectively.

## HuggingFace Diffusers in Practice

Now let's run some examples with the `diffusers` library. For this section, I have prepared a notebook:

[`diffusers/diffusers_and_co.ipynb`](https://github.com/mxagar/diffusion-examples/blob/main/diffusers/diffusers_and_co.ipynb)

In the blog post, I will show and comment the results of running different models; for detailed (and commented) code snippets, please check the notebook.

Note that if want to run the notebook you will need a [GPU setup with at least 12 GB of VRAM](https://mikelsagardia.io/blog/mac-os-ubuntu-nvidia-egpu.html); alternatively, you could launch a [Google Colab instance](https://colab.research.google.com/) with a NVIDIA T4, or similar.

The first notable example in the notebook deals with a *conditioned* image generation task, in particular `text-to-image` using the [Stable Diffusion XL Turbo](https://huggingface.co/stabilityai/sdxl-turbo) model. The code follows the pattern introduced in the previous section:

```python
from diffusers import AutoPipelineForText2Image

# Load the SDXL-Turbo text-to-image pipeline
pipe = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/sdxl-turbo", 
    torch_dtype=torch.float16, 
    variant="fp16"
)

prompt = """
A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook.
Soft light colors, landscape, peaceful, productive, and joyful atmosphere.
The robot is drawing an image of itself drawing, creating a recursive effect.
Large window in the background with greenery outside, warm natural lighting.
"""

# Seed for reproducibility
rand_gen = torch.manual_seed(148607185)

# Generate an image based on the text prompt
image = pipe(
    prompt=prompt, 
    num_inference_steps=1, # 1 for sdxl-turbo, 25-50 for SD
    guidance_scale=1.0, # 1 for sdxl-turbo, 6-10 for SD
    negative_prompt=["overexposed", "underexposed"], 
    generator=rand_gen
).images[0]
```

The result is promising, but it clearly reveals that the image was generated by a model: we can see that unfinished or rare details appear in eyes and fingers, the mechanical structure of the robot with straight geometrical lines is not consistent or seems unrealistic, etc.

<p align="center">
<img src="../assets/robot_painting_sdxl_turbo.png" alt="A friendly humanoid robot drawing itself." width="1000"/>
<small style="color:grey">
Image generated with <a href="https://huggingface.co/stabilityai/sdxl-turbo">SDXL Turbo</a>.
Prompt: <i>A friendly humanoid robot sits at a wooden table in a bright, sunlit room, happily drawing on a sketchbook. Soft light colors, landscape, peaceful, productive, and joyful atmosphere. The robot is drawing an image of itself drawing, creating a recursive effect. Large window in the background with greenery outside, warm natural lighting.</i>
</small>
</p>

[Stable Diffusion XL Turbo](https://huggingface.co/stabilityai/sdxl-turbo) is a real-time `text-to-image` diffusion model derived from [Stable Diffusion XL (SDXL)](https://arxiv.org/pdf/2307.01952), designed to generate high-quality images in as few as one to four denoising steps. Unlike traditional diffusion models that require dozens of inference steps, SDXL Turbo prioritizes latency and interactivity while preserving much of SDXL's visual fidelity.

SDXL Turbo is trained using [Adversarial Diffusion Distillation (ADD)](https://arxiv.org/abs/2311.17042). Instead of sampling from the full diffusion process during inference, the model learns to directly approximate the output of a multi-step teacher model (SDXL) in very few steps (1-4):

- A large, high-quality SDXL model acts as a teacher.
- The Turbo model is trained to match the teacher's output distribution.
- An adversarial objective helps close the quality gap caused by aggressive step reduction.

In other words, a bigger model is distilled to a smaller one; that smaller model enables real-time generation in UIs and creative tools.

An alternative model to SDXL is [Playground V2](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic), which also targets high-quality image generation with fewer inference steps; this model prioritizes visual quality and creative expressiveness, and doesn't follow any distillation during training.

<p align="center">
<img src="../assets/robot_painting_playground_v2.png" alt="A friendly humanoid robot drawing itself." width="1000"/>
<small style="color:grey">
Model: <a href="https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic">Playground V2</a>.
Same prompt ad before: <i>A friendly humanoid robot sits at a wooden table...</i>
</small>
</p>

Models can be used not only individually, but also in a concatenated chain. In the following example, I have let SDXL Turbo create an image of a puppy which is then used as condition for the `image-to-image` model [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-prior). The result is an exaggerated image of a dog, but I think it showcases the potential of building such chained pipelines.

<p align="center">
<img src="../assets/dog_drawing_sdlx_turbo_kandinsky.png" alt="A friendly humanoid dog." width="1000"/>
<small style="color:grey">
Left image generated by <a href="https://huggingface.co/stabilityai/sdxl-turbo">SDXL Tubo</a>.
Right image generated by <a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-prior">Kandinsky Prior 2.2</a>.
Left prompt: <i>A painting of a friendly dog painted by a child.</i>
Right prompt: <i>A photo of a friendly dog. High details, realistic (negative: low quality, bad quality).</i>
</small>
</p>

Kandinsky is a multimodal diffusion model that separates semantic understanding from image generation. Unlike SDXL-style models, which directly condition image generation on text embeddings, Kandinsky uses a two-stage architecture:

- Prior model, which maps text (and optionally images) into a shared latent space that represents high-level semantics.
- Decoder model (diffusion), which takes these semantic embeddings and generates the final image via a diffusion process.

This explicit separation makes Kandinsky particularly well suited for compositional pipelines. One such pipeline consists in *in-painting*, i.e.: we ask the model to generate a sub-image on a provided initial image. Here's how it works:

- Mask definition: A binary mask specifies which regions of the image should be regenerated (white) and which should remain fixed (black).
- Latent conditioning: The unmasked parts of the image are encoded and injected into the diffusion process, anchoring the generation spatially.
- Semantic guidance via the prior: Text prompts and optional image context guide what should appear in the masked regions.
- Diffusion-based regeneration: Noise is added only in the masked area, and the model denoises it while respecting both the surrounding visual context and the semantic intent from the prompt.

Because Kandinsky reasons at a semantic level first, inpainting results tend to be context-aware: lighting, perspective, and style are usually consistent with the original image, even when the prompt introduces new elements.

Here is an example with the popular oil painting [*The Girl with the Pearl Ear-ring* from Vermeer](https://en.wikipedia.org/wiki/Girl_with_a_Pearl_Earring). Unfortunately, the *pearl ear-ring* is removed!

<p align="center">
<img src="../assets/vermeer_girl_mask_inpainting_kandinsky.png" alt="A friendly humanoid dog." width="1000"/>
<small style="color:grey">
Model: <a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint">Kandinsky Inpaint 2.2</a>.
Prompt: <i>Oil painting of a woman wearing a surgical mask, Vermeer (negative: bad anatomy, deformed, ugly, disfigured).</i>
I obtained the image from the Wikipedia and draw the mask manually.
Check <a href="https://www.bbc.com/news/uk-england-bristol-52382500">this piece from Banksy</a>, if you would like to know how this could be done differently.
</small>
</p>

## Building Proof-of-Concept Applications: Zero-Shot Segmentation and In-Painting

As shown in the notebook [`diffusers/diffusers_and_co.ipynb`](https://github.com/mxagar/diffusion-examples/blob/main/diffusers/diffusers_and_co.ipynb), it is very simple to run different models for different isolated tasks. That brings us to the next logical question: *what if we combine different models to build small apps?*

Along these lines, I have implemented an [`inpainting_app`](https://github.com/mxagar/diffusion-examples/blob/main/diffusers), which works as follows:

- We can load an image, where we select points in a region we would like to segment: the foreground.
- Then, the [Segment Anything Model (SAM) from Meta](https://huggingface.co/docs/transformers/en/model_doc/sam) is used to create a mask of that region of interest; everything else is background. SAM is a vision transformer which is able to segment different parts of an image out-of-the-box. However, it requires some input points or a bounding box to specify which region to segment.
- Finally, we select either the foreground or the background region and run the in-painting version of [SDXL model](https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1); that way, the selected region is re-painted following the introduced prompt, but remaining consistent with the contents of the complementary region.

Note that, as before, if you plan to run the app locally you will need a [GPU setup with at least 12 GB of VRAM](https://mikelsagardia.io/blog/mac-os-ubuntu-nvidia-egpu.html) :sweat_smile:.

That application makes use of [Gradio](https://www.gradio.app/), a Python library similar to [Streamlit](https://streamlit.io/) which builds nice-looking, web-based GUIs. Gradio is developed by HuggingFace, making it the perfect choice for the models we are using. The library is really easy to use and I won't spend &mdash; if you are interested, you can check my [Gradio Quickstart Guide](https://github.com/mxagar/tool_guides/tree/master/gradio), which introduces all the necessary concepts and more.

The structure is quite simple:

- The GUI and the app structure are controlled by [`app.py`](https://github.com/mxagar/diffusion-examples/blob/main/inpainting_app/app.py). The entry point is `app.generate_app()`, which receives two functions:
    - The function that segments an image given some selection points.
    - The function which runs the inpainting of an image given a mask.
- The notebook [`inpainting.ipynb`](https://github.com/mxagar/diffusion-examples/blob/main/inpainting_app/inpainting.ipynb) defines and prepares those input functions:
    - `run_segmentation(raw_image, input_points, processor, model, ...) -> input_mask`
    - `run_inpainting(raw_image, input_mask, prompt, pipeline, ...) -> generated_image`
- Internally, `app.generate_app()` instantiates a `gradio.Blocks` object, which is composed by `gradio.Row()` sections that contain the UI widgets: image canvases, sliders, text boxes, buttons, etc. Those widgets are associated to callback functions that run the passed functions; for instance: when we select points in the uploaded `raw_image`, the callback `on_select()` is invoked, which under the hood executes `run_segmentation()` using the uploaded `raw_image` and the selected `input_points`.

Of course, we can pack everything into modules, but the notebook serves as a nice playground to test different functionalities and models interactively.

When we launch the application by invoking `app.generate_app()`, the user sees the following UI in `http://localhost:8080`:

<p align="center">
<img src="../inpainting_app/assets/app_gui.png" alt="App GUI." width="1000"/>
<small style="color:grey">
The Graphical User Interface (GUI) or our application.
</small>
</p>

So how does it perform? Let's see some examples!

<p align="center">
<img src="../inpainting_app/assets/monalisa_inpainting.png" alt="Monalisa In-Painting." width="1000"/>
<small style="color:grey">
Monalisa re-imagined. <a href="https://huggingface.co/docs/transformers/en/model_doc/sam">SAM (Segment Anything Model)</a> is used to segment foreground (green) & background (yellow), and <a href="https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1
">Stable Diffusion XL Inpainting</a> to re-generate the selected region.
Prompt (applied to the background): <i>A fantasy landscape with flying dragons (negative: artifacts, low quality, distortion).</i>
</small>
</p>

Conclusions...

## Wrapping Up

:construction: TBD.
